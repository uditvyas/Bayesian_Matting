{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18110176_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4teL8VmgBsy"
      },
      "source": [
        "# **Probability and Random Processes**\n",
        "# Assignment 3: Bayesian Matting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muLIf-YYftzn"
      },
      "source": [
        "import numpy as np\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.cluster import KMeans\n",
        "from numba import jit \n",
        "from tqdm import tqdm"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXy4kGp1gGAZ"
      },
      "source": [
        "### Downloading and Unzipping the input images, trimaps and ground truth from the source provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_xevXylgHHc"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"data\"):\n",
        "    shutil.rmtree(\"data\")\n",
        "\n",
        "!wget -q http://www.alphamatting.com/datasets/zip/input_training_lowres.zip\n",
        "!wget -q http://www.alphamatting.com/datasets/zip/trimap_training_lowres.zip\n",
        "!wget -q http://www.alphamatting.com/datasets/zip/gt_training_lowres.zip\n",
        "\n",
        "os.mkdir(\"data\")\n",
        "os.mkdir(\"data/Input\")\n",
        "os.mkdir(\"data/Trimap\")\n",
        "os.mkdir(\"data/GT\")\n",
        "\n",
        "!unzip -q \"input_training_lowres.zip\" -d \"data/Input\"\n",
        "!unzip -q \"trimap_training_lowres.zip\" -d \"data/Trimap\"\n",
        "!unzip -q \"gt_training_lowres.zip\" -d \"data/GT\"\n",
        "os.remove(\"input_training_lowres.zip\")\n",
        "os.remove(\"trimap_training_lowres.zip\")\n",
        "os.remove(\"gt_training_lowres.zip\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fgsrQC6gJtB"
      },
      "source": [
        "### Loading images and Trimaps in the program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0J4coIIgJaG"
      },
      "source": [
        "names = sorted(os.listdir(\"data/Input\"))\n",
        "\n",
        "input_images = []\n",
        "trimaps = []\n",
        "\n",
        "for name in names:\n",
        "    num = name.split('.')[0][-2:]\n",
        "    img = cv2.imread(\"data/Input/GT\"+num+\".png\")\n",
        "    input_images.append(img)\n",
        "    tri = cv2.imread(\"data/Trimap/Trimap1/GT\"+num+\".png\", cv2.IMREAD_GRAYSCALE)\n",
        "    trimaps.append(tri)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtR_DfAnJMSq"
      },
      "source": [
        "### All tunable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWNgJnajHpbA"
      },
      "source": [
        "### Tuning Parameters\n",
        "global sigma\n",
        "sigma = 10              # Default sigma for gaussian weights\n",
        "global N\n",
        "N = 25                  # Default window size\n",
        "global sigma_C\n",
        "sigma_C = 0.01          # Default variance for the pixel\n",
        "global maxIter\n",
        "maxIter = 1000          # Maximum number of iteration in iterative solving function (break condition)\n",
        "global MinLike\n",
        "MinLike = 1e-6          # Minimum change in iterative solving necessary for next iteration (break condition)\n",
        "global clust_var\n",
        "clust_var = 0.05        # Maximum variance of all the clusters allowed\n",
        "global MaxClusters\n",
        "MaxClusters = 6         # Maximum number of clusters in a given window\n",
        "global minN\n",
        "minN = 15               # Minimum number of neighbourhood pixels required for solving at given pixel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbQc_WlugWxk"
      },
      "source": [
        "### Helper Functions for generating gaussian distributions,  extracting window from the image and returning the weighted mean and covariances of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6tqF6fBgVgM"
      },
      "source": [
        "def gaussian_weights(size=N,sigma = sigma):\n",
        "    half = size//2\n",
        "    x , y = np.ogrid[-half:half+1,-half:half+1]\n",
        "    gaussian = np.exp(-(x**2 + y**2)/(2.0*sigma**2))\n",
        "    gaussian[gaussian<1e-14] = 0\n",
        "    return gaussian\n",
        "\n",
        "def window(image,x,y,N):\n",
        "    ## Same function for both alpha and the foreground/background. The only change\n",
        "    ## is the number of channels that are read and  returned in the window\n",
        "    halfN = N//2\n",
        "    if len(image.shape)==3:\n",
        "        h,w,c = image.shape\n",
        "        xmin = max(0, x - halfN); xmax = min(w, x + (halfN+1))\n",
        "        ymin = max(0, y - halfN); ymax = min(h, y + (halfN+1))\n",
        "        pxmin = halfN - (x-xmin); pxmax = halfN + (xmax-x)\n",
        "        pymin = halfN - (y-ymin); pymax = halfN + (ymax-y)    \n",
        "        window = np.zeros((N,N,c))\n",
        "        window[pymin:pymax, pxmin:pxmax] = image[max(0,y - halfN):min(h, y+halfN+1) ,max(0,x - halfN):min(w, x+halfN+1),:]\n",
        "        return window\n",
        "    elif len(image.shape)==2:\n",
        "        h,w = image.shape\n",
        "        xmin = max(0, x - halfN); xmax = min(w, x + (halfN+1))\n",
        "        ymin = max(0, y - halfN); ymax = min(h, y + (halfN+1))\n",
        "        pxmin = halfN - (x-xmin); pxmax = halfN + (xmax-x)\n",
        "        pymin = halfN - (y-ymin); pymax = halfN + (ymax-y)\n",
        "        window = np.zeros((N,N))\n",
        "        window[pymin:pymax, pxmin:pxmax] = image[max(0,y - halfN):min(h, y+halfN+1) ,max(0,x - halfN):min(w, x+halfN+1)]\n",
        "        return window\n",
        "\n",
        "## Numba jit is a tool for accelerating the performance of numpy functions\n",
        "@jit(nopython=True, cache=True)\n",
        "def weighted_mean_cov(pixels,weights):\n",
        "    W = np.sum(weights)\n",
        "    mean = (1/W) * np.dot(weights.T,pixels)\n",
        "    cov = (1/W) * np.dot(weights*(pixels-mean).T,(pixels-mean))\n",
        "    return mean, cov"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHfgpTW7HMl2"
      },
      "source": [
        "### The iterative function that solves iteratively for alpha, foreground and background at a pixel value, provided the pixels, weights and other parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbYE2bKXgSHm"
      },
      "source": [
        "@jit(nopython=True, cache=True)\n",
        "def solve_iterative(front_mu, front_cov, back_mu, back_cov, pixel, sigma_C, alpha_init, MaxIter, MinLike):\n",
        "    I = np.eye(3)\n",
        "    FMax = np.zeros(3)\n",
        "    BMax = np.zeros(3)\n",
        "    Max_alpha = 0\n",
        "    MaxLike = - np.inf\n",
        "    sigma_factor = 1/sigma_C**2\n",
        "\n",
        "    for i in range(front_mu.shape[0]):\n",
        "        mu_Fi = front_mu[i]\n",
        "        inverse_f = np.linalg.inv(front_cov[i])\n",
        "        for j in range(back_mu.shape[0]):\n",
        "            mu_Bj = back_mu[j]\n",
        "            inverse_b = np.linalg.inv(back_cov[j])\n",
        "\n",
        "            # Initialising the parameters\n",
        "            alpha = alpha_init\n",
        "            iter = 1\n",
        "            prev_like = - np.inf\n",
        "            \n",
        "            while True:\n",
        "                ## SETTING UP THE A AND B MATRICES\n",
        "                A11 = inverse_f + I*alpha**2 * sigma_factor\n",
        "                A12 = I*alpha*(1-alpha) * sigma_factor\n",
        "                A22 = inverse_b+I*(1-alpha)**2 * sigma_factor\n",
        "                A = np.vstack((np.hstack((A11, A12)), np.hstack((A12, A22))))\n",
        "\n",
        "                b1 = np.dot(inverse_f, mu_Fi) + pixel*(alpha) * sigma_factor\n",
        "                b2 = np.dot(inverse_b, mu_Bj) + pixel*(1-alpha) * sigma_factor\n",
        "                b = np.atleast_2d(np.concatenate((b1, b2))).T    # atleast_2d\n",
        "\n",
        "                # SOLVING FOR AX = B\n",
        "                X = np.linalg.solve(A, b)\n",
        "                F = np.maximum(0, np.minimum(1, X[0:3]))\n",
        "                B = np.maximum(0, np.minimum(1, X[3:6]))\n",
        "\n",
        "                alpha = np.maximum(0, np.minimum(1, np.dot((np.atleast_2d(pixel).T-B).T, (F-B))/np.sum((F-B)**2)))[0,0]\n",
        "\n",
        "                # CALCULATING LOG LIKELIHOOD AND ADDING THEM AND COMPARING WITH THE LAST ITERATION\n",
        "                L_pixel = - np.sum((np.atleast_2d(pixel).T -alpha*F-(1-alpha)*B)**2) * sigma_factor\n",
        "                L_F = (- ((F- np.atleast_2d(mu_Fi).T).T @ np.dot(inverse_f , (F-np.atleast_2d(mu_Fi).T)))/2)[0,0]\n",
        "                L_B = (- ((B- np.atleast_2d(mu_Bj).T).T @ np.dot(inverse_b , (B-np.atleast_2d(mu_Bj).T)))/2)[0,0]\n",
        "                this_like= (L_pixel + L_F + L_B)\n",
        "                \n",
        "                ## Checking if the log likelihood is increasing or not\n",
        "                if this_like> MaxLike:\n",
        "                    Max_alpha = alpha\n",
        "                    MaxLike = this_like\n",
        "                    FMax = F.flatten()\n",
        "                    BMax = B.flatten()\n",
        "\n",
        "                ## Breaking Condition, either reaching maximum iterations, or converging\n",
        "                if iter >= MaxIter or abs(this_like - prev_like) <= MinLike:\n",
        "                    break\n",
        "\n",
        "                prev_like = this_like\n",
        "                iter += 1\n",
        "    return FMax, BMax, Max_alpha"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygdpn5ixm_H8"
      },
      "source": [
        "### The clustering function (implemented using Scikit-learn KMeans Clustering)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py1Ag3W5gmYs"
      },
      "source": [
        "## CLUSTERING USING KMEANS CLUSTERING\n",
        "def Kcluster(pixels,weights, MinVar = clust_var, MaxClusters = MaxClusters):\n",
        "    n_clusters = 1\n",
        "    all_means = []\n",
        "    all_covs = []\n",
        "    mean, cov = weighted_mean_cov(pixels,weights)\n",
        "    cov = cov + 1e-5*np.eye(3)\n",
        "    all_means.append(mean)\n",
        "    all_covs.append(cov)\n",
        "    values, vectors = np.linalg.eig(cov)\n",
        "    max_value = np.max(values)\n",
        "    \n",
        "    ## Condition for forming one more cluster (if the variance of individual clusters is higher than MinVar)\n",
        "    while max_value>MinVar and n_clusters<MaxClusters:\n",
        "        n_clusters += 1\n",
        "        km = KMeans(n_clusters = n_clusters).fit(pixels)\n",
        "        all_max = []\n",
        "        for i in range(n_clusters):\n",
        "            idx = np.where(km.labels_ == i)[0]\n",
        "            cluster = pixels[idx]\n",
        "            weight = weights[idx]\n",
        "            mean, cov = weighted_mean_cov(cluster,weight)\n",
        "            cov = cov + 1e-5*np.eye(3)\n",
        "            all_means.append(mean)\n",
        "            all_covs.append(cov)\n",
        "            values, vectors = np.linalg.eig(cov)\n",
        "            all_max.append(np.max(values))\n",
        "        all_max = np.array(all_max)\n",
        "        max_value = np.max(all_max)\n",
        "    all_means = np.array(all_means)\n",
        "    all_covs = np.array(all_covs)\n",
        "    return all_means, all_covs\n",
        "     "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61fywjs6nPWo"
      },
      "source": [
        "### Bayesian matting function which takes an image and its trimap as the input and returns the alpha matte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ft2qmohgoiG"
      },
      "source": [
        "def Bayesian_matting(image,trimap,N=N, minN = minN):\n",
        "    h,w,c = image.shape\n",
        "    image = image/255\n",
        "    alpha = np.zeros((h,w))\n",
        "    \n",
        "    ## Separating the trimap into foreground, background and unknown regions\n",
        "    fg_mask = trimap == 255\n",
        "    bg_mask = trimap == 0\n",
        "    unknown_mask = trimap == 128\n",
        "\n",
        "    ## Getting the Guassian weights\n",
        "    weights = gaussian_weights(size = N)\n",
        "    g_weights = weights/np.max(weights)\n",
        "\n",
        "    ## Extracting the foregound based on the regions from trimap\n",
        "    foreground = image * np.repeat(fg_mask[:,:,np.newaxis],3, axis = 2)\n",
        "    background = image * np.repeat(bg_mask[:,:,np.newaxis],3, axis = 2)\n",
        "\n",
        "    ## Initialising the alpha matte\n",
        "    alpha[fg_mask] = 1\n",
        "    alpha[unknown_mask] = np.nan\n",
        "    n_unknown_pixels = np.sum(unknown_mask)\n",
        "\n",
        "    n = 1\n",
        "    unkreg = unknown_mask \n",
        "    kernel = np.ones((3, 3))\n",
        "\n",
        "    ## Initiaiting the while loop on all the unknown pixels\n",
        "    while n<n_unknown_pixels:\n",
        "        leftout = 0\n",
        "        unkreg = np.array(unkreg.astype(np.uint8))\n",
        "        unkreg = cv2.erode(unkreg, kernel, iterations=1)\n",
        "        unkpixels = np.logical_and(np.logical_not(unkreg), unknown_mask)\n",
        "        \n",
        "        Y, X = np.nonzero(unkpixels)\n",
        "        for i in range(Y.shape[0]):\n",
        "            if n % 1000 == 0:\n",
        "                print(n, n_unknown_pixels)\n",
        "                \n",
        "            y, x = Y[i], X[i]\n",
        "            p = image[y, x]\n",
        "            \n",
        "            ## Getting smaller windows to solve locally\n",
        "            a_window = window(alpha,x,y,N)\n",
        "            foreground_window = window(foreground,x,y,N)\n",
        "            background_window = window(background,x,y,N)\n",
        "            front = np.reshape(foreground_window, (N*N, 3))\n",
        "            back = np.reshape(background_window, (N*N, 3))\n",
        "            \n",
        "            ## Getting the correct weights by multiplying alpha and gaussian weights\n",
        "            front_weights = (a_window**2 * g_weights).flatten()\n",
        "            back_weights = ((1-a_window)**2 * g_weights).flatten()\n",
        "            \n",
        "            ## Extracting valid information of foreground and background\n",
        "            indices = np.nan_to_num(front_weights) > 0\n",
        "            front = front[indices,:]\n",
        "            front_weights = front_weights[indices]\n",
        "            \n",
        "            indices = np.nan_to_num(back_weights) > 0\n",
        "            back_weights = back_weights[indices]\n",
        "            back = back[indices,:]\n",
        "\n",
        "\n",
        "            ## Checking if the neighbourhood has sufficient information to solve, if not, skip the iteration\n",
        "            if len(front_weights) < minN or len(back_weights) < minN:\n",
        "                leftout += 1\n",
        "                continue\n",
        "\n",
        "            ## Clustering the colours to get weighted mean and variances\n",
        "            front_mu, front_cov = Kcluster(front, front_weights)\n",
        "            back_mu, back_cov = Kcluster(back, back_weights)\n",
        "            \n",
        "            ## Solving for F, B and alpha\n",
        "            alpha_init = np.nanmean(alpha.flatten())\n",
        "            F, B, Alpha = solve_iterative(front_mu, front_cov, back_mu, back_cov, p, sigma_C, alpha_init, maxIter, MinLike)\n",
        "\n",
        "            foreground[y, x] = F.flatten()\n",
        "            alpha[y, x] = Alpha\n",
        "            background[y, x] = B.flatten()\n",
        "            n += 1\n",
        "            unknown_mask[y, x] = 0\n",
        "        \n",
        "        ## Keeping track of the unknown pixels and therefore preventing the infinite loop by increasing the window size\n",
        "        remain = n_unknown_pixels - n\n",
        "        if remain == leftout-1:\n",
        "            print(\"Infinite Loop\")\n",
        "            N = N + 2\n",
        "            ## Additional condition reqiured in some images to prevent excessive smudging\n",
        "            # if N>60:\n",
        "            #     return alpha\n",
        "            weights = gaussian_weights(size = N)\n",
        "            g_weights = weights/np.max(weights)\n",
        "\n",
        "    return alpha"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_cZ9lUbnkiE"
      },
      "source": [
        "## Main function: Reads the images and trimaps one at a time, and saves two images\n",
        "1. Alpha Matte in original dimensions\n",
        "2. Plt figure with the associated error score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAqmaAwIUVMz",
        "outputId": "c65b1c1a-2923-4d4a-aa95-7067e7a4db4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Change the ranges in the range of the for loop to run matting for custom images\n",
        "# All alphas are submitted in the zip file along with the report\n",
        "for img_num in range(11,12):\n",
        "    GT = cv2.imread(\"/content/data/GT/GT{:02d}.png\".format(img_num),cv2.IMREAD_GRAYSCALE).flatten()\n",
        "    alpha = 255 * Bayesian_matting(input_images[img_num-1],trimaps[img_num-1])\n",
        "    cv2.imwrite(\"alpha_{}.png\".format(img_num),alpha)\n",
        "    alpha = cv2.imread(\"alpha_{}.png\".format(img_num), cv2.IMREAD_GRAYSCALE)\n",
        "    total = len(GT)\n",
        "\n",
        "    plt.imshow(alpha, cmap = 'gray')\n",
        "    plt.title(\"Alpha matte Score: {:.2f}\".format(np.sum(np.absolute(alpha.flatten() - GT))/total))\n",
        "    plt.axis('off')\n",
        "    plt.savefig(\"Output_{}.png\".format(img_num))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 61930\n",
            "2000 61930\n",
            "3000 61930\n",
            "4000 61930\n",
            "5000 61930\n",
            "6000 61930\n",
            "7000 61930\n",
            "8000 61930\n",
            "9000 61930\n",
            "10000 61930\n",
            "11000 61930\n",
            "12000 61930\n",
            "13000 61930\n",
            "14000 61930\n",
            "15000 61930\n",
            "16000 61930\n",
            "17000 61930\n",
            "18000 61930\n",
            "19000 61930\n",
            "20000 61930\n",
            "21000 61930\n",
            "22000 61930\n",
            "23000 61930\n",
            "24000 61930\n",
            "25000 61930\n",
            "26000 61930\n",
            "27000 61930\n",
            "28000 61930\n",
            "29000 61930\n",
            "30000 61930\n",
            "31000 61930\n",
            "32000 61930\n",
            "33000 61930\n",
            "34000 61930\n",
            "35000 61930\n",
            "36000 61930\n",
            "37000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "38000 61930\n",
            "39000 61930\n",
            "40000 61930\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}